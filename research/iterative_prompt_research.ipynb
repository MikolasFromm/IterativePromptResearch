{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Prompt Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sources and their cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node and Leaf definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name : str, predecessor : 'Node' = None, alternative_name : str = None):\n",
    "        self.name = name\n",
    "        self.predecessor = predecessor\n",
    "        self.alternative_name : str = alternative_name\n",
    "        self.children : List['Node'] = []\n",
    "\n",
    "    def get_child(self, index : int):\n",
    "        if (index >= len(self.children)):\n",
    "            assert IndexError\n",
    "        return self.children[index]\n",
    "    \n",
    "    def add_children(self, children : List['Node']):\n",
    "        for child in children:\n",
    "            child.predecessor = self\n",
    "            self.children.append(child)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.name}, #children: {len(self.children)}\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n",
    "\n",
    "class Leaf(Node):\n",
    "    def __init__(self, name : str, predecessor : 'Node' = None, alternative_name : str = None):\n",
    "        super().__init__(name, predecessor, alternative_name)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.name}\"\n",
    "    \n",
    "class CorrectPath:\n",
    "    def __init__(self, root : Node, query : str, child_sequence : List['int']):\n",
    "        ## sequence must be given in reversed order, having the first move at the last position\n",
    "        self.root = root\n",
    "        self.query = query\n",
    "        self.child_sequence = child_sequence \n",
    "    \n",
    "    def is_next_move_correct(self, move_index : int) -> bool:\n",
    "        if (len(self.child_sequence) == 0): return False ## if no child given, I am already at the correct node\n",
    "        if (move_index == self.child_sequence[-1]): return True\n",
    "        else: return False\n",
    "        \n",
    "    def make_correct_step(self) -> Node | None:\n",
    "        if (len(self.child_sequence) == 0): return None\n",
    "        self.root = self.root.get_child(self.child_sequence[-1])\n",
    "        self.child_sequence = self.child_sequence[:-1]\n",
    "        return self\n",
    "\n",
    "def create_correct_sequence(root : Node, query : str, child_sequence_str : str) -> 'CorrectPath':\n",
    "    child_sequence = [int(x) for x in child_sequence_str.split()]\n",
    "    child_sequence.reverse()\n",
    "    return CorrectPath(root, query, child_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WorldBank - XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "worldbank_namespaces =  {\n",
    "        'nt': 'urn:eu.europa.ec.eurostat.navtree'\n",
    "    }\n",
    "\n",
    "def parse_worldbank_xml(path : str) -> Node:\n",
    "    \"\"\"Parses the xml tree from the given path and returns the root node.\"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    dataset : Node = Node(\"WorldBank\")\n",
    "    dataset.add_children(get_node_children(root, dataset))\n",
    "    return dataset\n",
    "\n",
    "def get_node_children(root : ET.Element, predecessor_node : Node) -> List['Node']:\n",
    "    \"\"\"Parses the given root node and returns the corresponding DataSet object.\"\"\"\n",
    "    datasets = []\n",
    "    for branch in root.findall('nt:branch', worldbank_namespaces):\n",
    "        title = branch.find('nt:title/[@language=\"en\"]', worldbank_namespaces).text\n",
    "        branch_dataset = Node(title, predecessor_node)\n",
    "        for child in branch.findall('nt:children', worldbank_namespaces):\n",
    "            branch_dataset.children = get_node_children(child, branch_dataset)\n",
    "        datasets.append(branch_dataset)\n",
    "\n",
    "    for leaf in root.findall('nt:leaf', worldbank_namespaces):\n",
    "        title = leaf.find('nt:title/[@language=\"en\"]', worldbank_namespaces).text\n",
    "        datasets.append(Leaf(title, predecessor_node))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorldBank, #children: 3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldbank : Node = None\n",
    "if (os.path.exists(\"worldbank.pkl\")):\n",
    "    worldbank = pickle.load(open(\"worldbank.pkl\", \"rb\"))\n",
    "else:\n",
    "    worldbank = parse_worldbank_xml(\"worldBank_content.xml\")\n",
    "    pickle.dump(worldbank, open(\"worldbank.pkl\", \"wb\"))\n",
    "\n",
    "worldbank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WebPage - HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from bs4 import BeautifulSoup\n",
    "from copy import deepcopy\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "SLEEP_TIME = 5\n",
    "TIMEOUT = 10\n",
    "DEPTH = 8\n",
    "ATTEMPTS_COUNT = 3\n",
    "\n",
    "def parse_html_webpage(path : str) -> Node:\n",
    "    \"\"\"Parses the html webpage into a tree and returns the root node.\"\"\"\n",
    "    dataset = Node(\"MFF home page\", None, path)\n",
    "    dataset.add_children(get_html_children(dataset, DEPTH))\n",
    "    return dataset\n",
    "\n",
    "def get_html_children(predecessor_node : Node, remaining_depth : int, cache : Dict[tuple[str, int], Node] = {}) -> List['Node']:\n",
    "    \"\"\"Parses the given soup and returns the corresponding DataSet object.\"\"\"\n",
    "    \"\"\"If item found in the cachce, it will be returned from there. Key of the cache is the url and the remaining depth.\"\"\"\n",
    "    \"\"\"I can always cut the depth of the children to the required level, but I cannot expand the children if the depth is not enough.\"\"\"\n",
    "    child_webpages = []\n",
    "    if (remaining_depth <= 0): return child_webpages\n",
    "\n",
    "    attempt_count = 1\n",
    "    page : requests.Response = None\n",
    "    soup : BeautifulSoup = None\n",
    "    # print(f\"Remain depth: {remaining_depth} : {predecessor_node.alternative_name}\")\n",
    "    while (attempt_count <= ATTEMPTS_COUNT):\n",
    "        try:\n",
    "            time.sleep(SLEEP_TIME)\n",
    "            page = requests.get(predecessor_node.alternative_name, timeout=TIMEOUT)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            break\n",
    "        # except:\n",
    "        #     print(f\"Attempt {attempt_count}/{ATTEMPTS_COUNT} failed...\")\n",
    "        finally:\n",
    "            attempt_count += 1\n",
    "    \n",
    "    if (page == None): return child_webpages\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        if ('href' in link.attrs and link.text.strip() != '' and not url_is_blacklisted(link.attrs['href'])):\n",
    "            url = url_get_absolute(link.attrs['href'], predecessor_node.alternative_name)\n",
    "            ## check if any key with same or higher depth_remaining is in the cache\n",
    "            key : tuple[str,int] = (url, next((x for x in range(remaining_depth-1, DEPTH, 1) if (url, x) in cache), None))\n",
    "            if (key[1] is not None): ## if anything found in the cache\n",
    "                branch_dataset = deepcopy(cache[key]) ## copy the cached item\n",
    "                cut_children_depth(branch_dataset, remaining_depth-1) ## cut the depth to the required level\n",
    "                branch_dataset.predecessor = predecessor_node ## set the predecessor\n",
    "                branch_dataset.alternative_name = url ## set the alternative name to its url\n",
    "            else:\n",
    "                title = re.sub('[\\\\n\\\\s]+',' ',link.text.strip())\n",
    "                branch_dataset = Node(title, predecessor_node, url) ## create new item\n",
    "                branch_dataset.add_children(get_html_children(branch_dataset, remaining_depth - 1, cache)) ## expand the children\n",
    "                ## remove all chached items with same url and lower depth\n",
    "                if (any(k[0] == url and k[1] < remaining_depth for k in cache.keys())):\n",
    "                    cache = {k:v for k,v in cache.items() if k[0] != url or k[1] > remaining_depth}\n",
    "                cache[(url, remaining_depth-1)] = branch_dataset\n",
    "            child_webpages.append(branch_dataset) ## add to the children response\n",
    "    return child_webpages\n",
    "\n",
    "def url_is_blacklisted(url : str, base_url : str | None = None) -> bool:\n",
    "    \"\"\"Checks if the url is blacklisted. Blacklist is very basic.\"\"\"\n",
    "    ## keep relatives\n",
    "    if (url.startswith('./') or (url != \"/\" and url.startswith('/'))):\n",
    "        return False\n",
    "    \n",
    "    ## filter out some basic stuff\n",
    "    if (url == \"/\" \n",
    "        or url.startswith('#') \n",
    "        or url.startswith('mailto:') \n",
    "        or url.startswith('javascript:')\n",
    "        or url.startswith('tel:')\n",
    "        or (base_url is not None and not url.startswith(base_url))):\n",
    "        return True\n",
    "\n",
    "    ## passed\n",
    "    return False\n",
    "\n",
    "def url_get_absolute(input : str, current_url : str | None) -> str:\n",
    "        \"\"\"Tries to merge the relative input url with the current url prefix to get the absolute url. \n",
    "        If no current url is provided, the input is returned.\"\"\"\n",
    "        result = \"\"\n",
    "        \n",
    "        if (current_url is None): return input\n",
    "        ## remove the query string from the url\n",
    "        if ('?' in current_url): current_url = current_url.split('?')[0]\n",
    "        ## remove any anchors from the url\n",
    "        if ('#' in input): input = input.split('#')[0]\n",
    "\n",
    "        ## try to cut as much from the current url as possible\n",
    "        if (input.startswith('/')): \n",
    "            current_split = [x for x in current_url.split('/') if x != '']\n",
    "            input_split = [x for x in input.split('/') if x != '']\n",
    "            for i in range(len(current_split)):\n",
    "                if (len(input_split) == 0): return current_url\n",
    "                if (current_split[i] == input_split[0]): input_split.pop(0)\n",
    "            input = '/'.join(input_split)\n",
    "\n",
    "            if (current_url.endswith('/')): result = current_url + input\n",
    "            else: result = current_url + '/' + input\n",
    "        ## just append the relative to the current\n",
    "        elif (input.startswith('./')): result = current_url + input.strip('./')\n",
    "        ## otherwise legit URL given\n",
    "        else: result = input\n",
    "        ## always add the trailing slash if not present\n",
    "        if (not result.endswith('/') and not \".\" in result.split('/')[-1]): ## only except if file is given\n",
    "            result += '/'\n",
    "\n",
    "        return result\n",
    "\n",
    "def cut_children_depth(node : Node, depth : int):\n",
    "    \"\"\"Cuts the tree depth to the given depth.\"\"\"\n",
    "    if (depth <= 0):\n",
    "        node.children = []\n",
    "        return\n",
    "    for child in node.children:\n",
    "        cut_children_depth(child, depth - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage : Node = None\n",
    "if (os.path.exists(\"webpage.pkl\")):\n",
    "    webpage = pickle.load(open(\"webpage.pkl\", \"rb\"))\n",
    "else:\n",
    "    webpage = parse_html_webpage(\"https://www.mff.cuni.cz/\")\n",
    "    pickle.dump(webpage, open(\"webpage.pkl\", \"wb\"))\n",
    "\n",
    "webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FileSystem - HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "\n",
    "def parse_html_file(path : str) -> Node:\n",
    "    \"\"\"Parses the html file into a tree and returns the root node.\"\"\"\n",
    "    with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        return get_html_file_children(soup)\n",
    "    \n",
    "def get_html_file_children(soup : BeautifulSoup) -> Node:\n",
    "    stack = deque()\n",
    "    initial_node : Node = None\n",
    "    previous_node : Node = None\n",
    "    for link in soup.find_all('a'):\n",
    "        if ('href' in link.attrs and link.text.strip() != ''):\n",
    "            url : str = link.attrs['href']\n",
    "            dir : str = url[:url.rindex('/')]\n",
    "            file : str = url[url.rindex('/')+1:]\n",
    "            text : str = link.text.strip()\n",
    "\n",
    "            if (initial_node is None):\n",
    "                initial_node = Node(text, None, dir)\n",
    "                previous_node = initial_node\n",
    "                continue\n",
    "\n",
    "            if (file == \"\" and  dir.startswith(previous_node.alternative_name)): ## when the file is empty, it is a directory\n",
    "                current_node = Node(text, previous_node, dir)\n",
    "                previous_node.children.append(current_node)\n",
    "                stack.append(previous_node)\n",
    "                previous_node = current_node\n",
    "            elif (file != \"\" and dir.startswith(previous_node.alternative_name)): \n",
    "                previous_node.children.append(Leaf(file, previous_node, dir))\n",
    "            else:\n",
    "                while (len(stack) > 0 and not dir.startswith(previous_node.alternative_name)):\n",
    "                    previous_node = stack.pop()\n",
    "                if (file == \"\"): ## when the file is empty, it is a directory\n",
    "                    current_node = Node(text, previous_node, dir)\n",
    "                    previous_node.children.append(current_node)\n",
    "                    stack.append(previous_node)\n",
    "                    previous_node = current_node\n",
    "                elif (file != \"\" and dir.startswith(previous_node.alternative_name)): \n",
    "                    previous_node.children.append(Leaf(file, previous_node, dir))\n",
    "    return initial_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOCUMENTS, #children: 43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesystem : Node = None\n",
    "if (os.path.exists(\"filesystem.pkl\")):\n",
    "    filesystem = pickle.load(open(\"filesystem.pkl\", \"rb\"))\n",
    "else:\n",
    "    filesystem = parse_html_file(\"filesystem_content.html\")\n",
    "    pickle.dump(filesystem, open(\"filesystem.pkl\", \"wb\"))\n",
    "\n",
    "filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Assistant - ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openAI_secret import API_KEY\n",
    "from consts import MODEL\n",
    "\n",
    "class OpenAIWrapper:\n",
    "    def __init__(self, keep_whole_context : bool, system_message : str):\n",
    "        self.client = OpenAI(api_key=API_KEY)\n",
    "        self.keep_whole_context = keep_whole_context\n",
    "        self.system_message = system_message\n",
    "        self.messages = [self.__create_system_message(self.system_message)]\n",
    "\n",
    "    def __create_user_message(self, content):\n",
    "        return {\"role\": \"user\", \"content\": content}\n",
    "    \n",
    "    def __create_system_message(self, content):\n",
    "        return {\"role\": \"system\", \"content\": content}\n",
    "    \n",
    "    def __create_assistant_message(self, content):\n",
    "        return {\"role\": \"assistant\", \"content\": content}\n",
    "    \n",
    "    def __add_message(self, content):\n",
    "        if (self.keep_whole_context):\n",
    "            self.messages.append(self.__create_user_message(content))\n",
    "        else:\n",
    "            self.messages = [\n",
    "                self.__create_system_message(self.system_message),\n",
    "                self.__create_user_message(content) \n",
    "            ]\n",
    "\n",
    "    def __get_response(self):\n",
    "        full_response = self.client.chat.completions.create(\n",
    "            model = MODEL,\n",
    "            messages=self.messages,\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        response_content = full_response.choices[-1].message.content\n",
    "        self.messages.append(self.__create_assistant_message(response_content))\n",
    "        return response_content\n",
    "    \n",
    "    def get_response_to_prompt(self, prompt) -> str:\n",
    "        self.__add_message(prompt)\n",
    "        response = self.__get_response()\n",
    "        return response if response is not None else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from consts import *\n",
    "\n",
    "class AssistantWorker:\n",
    "    def __init__(self, look_ahead_depth : int = 1):\n",
    "        self.stemmer : SnowballStemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        self.look_ahead_depth = look_ahead_depth\n",
    "        pass\n",
    "\n",
    "    def create_llm_query(self, initial_query : str, path_done : List['Node'], current_node : Node, mode : int) -> str:\n",
    "        \"\"\"Creates a query for the LLM model based on the current node.\"\"\"\n",
    "        next_moves = []\n",
    "        prompt = \"\"\n",
    "        steps_so_far = \" -> \".join(path_done)\n",
    "\n",
    "        match mode:\n",
    "            case WORKER_MODE.STEP_BY_STEP | WORKER_MODE.MATCH_AND_FILTER:\n",
    "                next_moves = [f\"\\t{i}: {child.name}\\n\" for i, child in enumerate(current_node.children)]\n",
    "                prompt = (\n",
    "                f\"query: {initial_query}\\n\"\n",
    "                f\"steps done: {steps_so_far}\\n\"\n",
    "                f\"next possible subsection names:\\n\"\n",
    "                f\"{''.join(next_moves)}\"\n",
    "                )\n",
    "            case WORKER_MODE.LOOK_AHEAD:\n",
    "                for i, child in enumerate(current_node.children):\n",
    "                    next_moves.append(f\"{i}: {self.create_look_ahead_prompt(child, self.look_ahead_depth-1, self.look_ahead_depth-1)}\")\n",
    "                prompt = (\n",
    "                f\"query: {initial_query}\\n\"\n",
    "                f\"steps done: {steps_so_far}\\n\"\n",
    "                f\"next possible subsection names:\\n\"\n",
    "                f\"{''.join(next_moves)}\"\n",
    "                )\n",
    "            case WORKER_MODE.KEYWORD_GEN_AND_MATCH:\n",
    "                prompt = (\n",
    "                f\"query: {initial_query}\\n\"\n",
    "                f\"For the query above, please write {NUM_OF_KEYWORDS} keywords that might be relevant names of subsections to dive into in an upcoming search. Prefere single words. Use the language of the query!\\n\"\n",
    "                f\"Please separate the keywords with semicolon. Dont write anything else!\\n\"\n",
    "                )\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "    def create_llm_nonint_message() -> str:\n",
    "        response = f\"You must only return integer, for example 3. If you don't know, return -1\"\n",
    "        return response\n",
    "\n",
    "    def process_llm_child_pick_response(self, raw_response : str) -> int | None:\n",
    "        \"\"\"Returns the child index of the llm decision. Returns -1 if doesn't, returns NONE if non-digit answer given.\"\"\"\n",
    "        response = raw_response.strip()\n",
    "\n",
    "        if (response == \"-1\"):\n",
    "            return -1\n",
    "        \n",
    "        if not response.isdigit():\n",
    "            print(f\"Response is not digit, raw response: {raw_response}\")\n",
    "            return None\n",
    "        else:\n",
    "            try:\n",
    "                int_response = int(response)\n",
    "                return int_response\n",
    "            except:\n",
    "                print(f\"Invalid response, raw response: {raw_response}\")\n",
    "                return None\n",
    "\n",
    "    def process_llm_keyword_gen_response(self, raw_response : str) -> List['str']:\n",
    "        return [self.stemmer.stem(x.strip().casefold()) for x in raw_response.split(\";\")]\n",
    "    \n",
    "    def create_look_ahead_prompt(self, child : Node, depth_total : int, depth_remaining : int):\n",
    "        tab = '\\t'\n",
    "        newline = '\\n'\n",
    "        prompt = f\"{''.join([tab for _ in range(depth_total - depth_remaining)])}: {child.name}{newline}\"\n",
    "        \n",
    "        if depth_remaining == 0:\n",
    "            return prompt\n",
    "        \n",
    "        for i, grandchild in enumerate(child.children):\n",
    "            prompt += f\"{self.create_look_ahead_prompt(grandchild, depth_total, depth_remaining - 1)}\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test runners"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
