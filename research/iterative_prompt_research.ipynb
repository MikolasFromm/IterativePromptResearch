{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Prompt Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sources and their cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name : str, predecessor : 'Node' = None, alternative_name : str = None):\n",
    "        self.name = name\n",
    "        self.predecessor = predecessor\n",
    "        self.alternative_name : str = None\n",
    "        self.children = []\n",
    "\n",
    "    def get_child(self, index : int):\n",
    "        if (index >= len(self.children)):\n",
    "            assert IndexError\n",
    "        return self.children[index]\n",
    "    \n",
    "    def add_children(self, children : List['Node']):\n",
    "        for child in children:\n",
    "            child.predecessor = self\n",
    "            self.children.append(child)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.name}, #children: {len(self.children)}\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n",
    "\n",
    "class Leaf(Node):\n",
    "    def __init__(self, name : str, predecessor : 'Node' = None, alternative_name : str = None):\n",
    "        super().__init__(name, predecessor, alternative_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WorldBank - XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "worldbank_namespaces =  {\n",
    "        'nt': 'urn:eu.europa.ec.eurostat.navtree'\n",
    "    }\n",
    "\n",
    "def parse_worldbank_xml(path : str) -> Node:\n",
    "    \"\"\"Parses the xml tree from the given path and returns the root node.\"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    dataset : Node = Node(\"WorldBank\")\n",
    "    dataset.add_children(get_node_children(root, dataset))\n",
    "    return dataset\n",
    "\n",
    "def get_node_children(root : ET.Element, predecessor_node : Node) -> List['Node']:\n",
    "    \"\"\"Parses the given root node and returns the corresponding DataSet object.\"\"\"\n",
    "    datasets = []\n",
    "    for branch in root.findall('nt:branch', worldbank_namespaces):\n",
    "        title = branch.find('nt:title/[@language=\"en\"]', worldbank_namespaces).text\n",
    "        branch_dataset = Node(title, predecessor_node)\n",
    "        for child in branch.findall('nt:children', worldbank_namespaces):\n",
    "            branch_dataset.children = get_node_children(child, branch_dataset)\n",
    "        datasets.append(branch_dataset)\n",
    "\n",
    "    for leaf in root.findall('nt:leaf', worldbank_namespaces):\n",
    "        title = leaf.find('nt:title/[@language=\"en\"]', worldbank_namespaces).text\n",
    "        datasets.append(Leaf(title, predecessor_node))\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorldBank, #children: 3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_worldbank_xml(\"worldBank_content.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebPage - HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "TIMEOUT = 10\n",
    "DEPTH = 2\n",
    "\n",
    "def parse_html_webpage(path : str) -> Node:\n",
    "    \"\"\"Parses the html webpage into a tree and returns the root node.\"\"\"\n",
    "    dataset : Node = Node(\"MFF home page\", None, path)\n",
    "    dataset.add_children(get_html_children(dataset, DEPTH))\n",
    "    return dataset\n",
    "\n",
    "def get_html_children(predecessor_node : Node, remaining_depth : int) -> List['Node']:\n",
    "    \"\"\"Parses the given soup and returns the corresponding DataSet object.\"\"\"\n",
    "    datasets = []\n",
    "    if (remaining_depth <= 0): return datasets\n",
    "\n",
    "    print(f\"Requesting {predecessor_node.alternative_name}\")\n",
    "    page = requests.get(predecessor_node.alternative_name, timeout=TIMEOUT)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        if ('href' in link.attrs and link.text.strip() != '' and not url_is_blacklisted(link.attrs['href'])):\n",
    "            url = url_get_absolute(link.attrs['href'], predecessor_node.alternative_name)\n",
    "            title = re.sub('[\\\\n\\\\s]+',' ',link.text.strip())\n",
    "            branch_dataset = Node(title, predecessor_node, url)\n",
    "            branch_dataset.children = get_html_children(link, branch_dataset, remaining_depth - 1)\n",
    "            datasets.append(branch_dataset)\n",
    "    return datasets\n",
    "\n",
    "def url_is_blacklisted(url : str, base_url : str | None = None) -> bool:\n",
    "    \"\"\"Checks if the url is blacklisted. Blacklist is very basic.\"\"\"\n",
    "    ## keep relatives\n",
    "    if (url.startswith('./') or (url != \"/\" and url.startswith('/'))):\n",
    "        return False\n",
    "    \n",
    "    ## filter out some basic stuff\n",
    "    if (url == \"/\" \n",
    "        or url.startswith('#') \n",
    "        or url.startswith('mailto:') \n",
    "        or url.startswith('javascript:')\n",
    "        or url.startswith('tel:')\n",
    "        or (base_url is not None and not url.startswith(base_url))):\n",
    "        return True\n",
    "\n",
    "    ## passed\n",
    "    return False\n",
    "\n",
    "def url_get_absolute(input : str, current_url : str | None) -> str:\n",
    "        \"\"\"Tries to merge the relative input url with the current url prefix to get the absolute url. \n",
    "        If no current url is provided, the input is returned.\"\"\"\n",
    "        result = \"\"\n",
    "        \n",
    "        if (current_url is None): return input\n",
    "        ## remove the query string from the url\n",
    "        if ('?' in current_url): current_url = current_url.split('?')[0]\n",
    "        ## remove any anchors from the url\n",
    "        if ('#' in input): input = input.split('#')[0]\n",
    "\n",
    "        ## try to cut as much from the current url as possible\n",
    "        if (input.startswith('/')): \n",
    "            current_split = [x for x in current_url.split('/') if x != '']\n",
    "            input_split = [x for x in input.split('/') if x != '']\n",
    "            for i in range(len(current_split)):\n",
    "                if (len(input_split) == 0): return current_url\n",
    "                if (current_split[i] == input_split[0]): input_split.pop(0)\n",
    "            input = '/'.join(input_split)\n",
    "\n",
    "            if (current_url.endswith('/')): result = current_url + input\n",
    "            else: result = current_url + '/' + input\n",
    "        ## just append the relative to the current\n",
    "        elif (input.startswith('./')): result = current_url + input.strip('./')\n",
    "        ## otherwise legit URL given\n",
    "        else: result = input\n",
    "        ## always add the trailing slash if not present\n",
    "        if (not result.endswith('/') and not \".\" in result.split('/')[-1]): ## only except if file is given\n",
    "            result += '/'\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MFF home page, #children: 170"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_html_webpage(\"https://www.mff.cuni.cz/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FileSystem - HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DEPTH = 2\n",
    "\n",
    "def parse_html_file(path : str) -> Node:\n",
    "    \"\"\"Parses the html file into a tree and returns the root node.\"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        dataset : Node = Node(\"MFF home page\", None, path)\n",
    "        dataset.add_children(get_html_children(soup, dataset, DEPTH))\n",
    "        return dataset\n",
    "    \n",
    "def get_html_file_children(soup : BeautifulSoup, predecessor_node : Node, remaining_depth : int) -> List['Node']:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
